{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b8a311",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a38da84",
   "metadata": {},
   "source": [
    "Simple Linear Regression (SLR) is a type of linear regression model that deals with only one predictor variable. Multiple Linear Regression (MLR) is a type of linear regression model that deals with more than one predictor variable.\n",
    "\n",
    "SLR Example:\n",
    "\n",
    "|Height |Weight| \n",
    "|-----|:-----|\n",
    "|175 |80  |\n",
    "|180|85|\n",
    "|185|90|\n",
    "\n",
    "We can create a simple linear regression model by drawing a straight line that best fits the data points. This line represents the relationship between height and weight.\n",
    "\n",
    "MLR Example:\n",
    "\n",
    "|Height |Weight| BMI|\n",
    "|-----|:-----|:---:|\n",
    "|175 |80  |24.5|\n",
    "|180|85|25.5|\n",
    "|185|90|26.5|\n",
    "\n",
    "In MLR, we can create a model that takes into account multiple predictor variables, in this case, height, weight, and BMI. The line that best fits the data points represents the relationship between weight and height, considering BMI as a moderating variable.\n",
    "\n",
    "In both SLR and MLR, the objective is to find the best-fitting line (or curve) that represents the relationship between the response variable and the predictor variables. However, MLR can provide a more comprehensive understanding of the relationship, considering the influence of multiple predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc851b",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96122f",
   "metadata": {},
   "source": [
    "Linear regression makes several assumptions about the data. Violations of these assumptions may affect the reliability and validity of the regression model. \n",
    "\n",
    "Key assumptions of linear regression:\n",
    "\n",
    "1. **Linearity:** The relationship between the independent variables and the dependent variable is assumed to be linear. This means that changes in the independent variables are associated with constant and proportional changes in the dependent variable.\n",
    "\n",
    "2. **Independence:** The residuals (the differences between the observed and predicted values) should be independent of each other. In other words, the value of the dependent variable for one observation should not be influenced by the values of the dependent variable for other observations.\n",
    "\n",
    "3. **Homoscedasticity:** The variance of the residuals should be constant across all levels of the independent variables. In simpler terms, the spread of the residuals should be consistent as you move along the predicted values.\n",
    "\n",
    "4. **Normality of Residuals:** The residuals should be approximately normally distributed. This assumption is important for making inferences and constructing confidence intervals.\n",
    "\n",
    "5. **No Perfect Multicollinearity:** In multiple linear regression, the independent variables should not be perfectly correlated with each other. High correlations can lead to unstable coefficient estimates.\n",
    "\n",
    "Checking these assumptions is crucial to ensure the reliability of the linear regression model.\n",
    "\n",
    "Ways to assess these assumptions:\n",
    "\n",
    "1. **Residual Analysis:** Examine the residuals by plotting them against the predicted values. The residuals should show a random pattern with no clear trends. If there are patterns, it suggests a violation of the independence assumption.\n",
    "\n",
    "2. **Normality Tests:** Use statistical tests or graphical methods (e.g., Q-Q plots) to assess the normality of residuals. While normality is not strictly necessary for large sample sizes, it's still helpful.\n",
    "\n",
    "3. **Homoscedasticity Check:** Plot the residuals against the predicted values and look for a consistent spread. A funnel-shaped pattern may indicate heteroscedasticity.\n",
    "\n",
    "4. **VIF (Variance Inflation Factor):** Check for multicollinearity by calculating the VIF for each independent variable. VIF values above a certain threshold (commonly 10) may indicate problematic multicollinearity.\n",
    "\n",
    "5. **Durbin-Watson Statistic:** This statistic helps detect autocorrelation in the residuals. A value around 2 suggests no autocorrelation, while values significantly deviating from 2 may indicate a problem.\n",
    "\n",
    "6. **Cook's Distance:** Identify influential data points that may have a significant impact on the regression coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be6cac",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e4f50",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept play a crucial role in interpreting the model. The slope, represented by the parameter β, represents the change in the response variable (y) for a one-unit change in the predictor variable (x). The intercept, represented by the parameter β0, represents the value of the response variable when the predictor variable is 0.\n",
    "\n",
    "Consider the following linear regression model: y = 2x + 3.\n",
    "\n",
    "The slope in this model is 2, indicating that for every one-unit increase in x, the corresponding increase in y is 2 units. The intercept in this model is 3, indicating that when x is 0, y is 3.\n",
    "\n",
    "Let's assume we have the following data:\n",
    "\n",
    "|Height(x) |Weight(y)| \n",
    "|-----|:-----|\n",
    "|60 |120  |\n",
    "|65|130|\n",
    "|70|140|\n",
    "\n",
    "When we plot these data points and fit a line with the equation y = 2x + 3, the line that best fits the data represents the relationship between height and weight. The slope of the line is 2, indicating that for every one additional inch of height, weight increases by 2 pounds. The intercept of the line is 3, indicating that when height is 0 inches (which is not a valid height), weight is 3 pounds.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d6cb0",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c666beb",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm that's used when training a machine learning model. It aims to find the best-fitting parameters for a given model by minimizing the cost function, which measures the error between the predicted outputs and the actual outputs.\n",
    "\n",
    "The gradient descent algorithm works by taking the negative gradient of the cost function, which indicates the direction of the steepest decrease. This negative gradient is then used to update the model's parameters, with the goal of minimizing the cost function. This process is repeated iteratively until the algorithm converges to a local minimum of the cost function.\n",
    "\n",
    "Steps involved to descibe how gradient descent is used in machine learning:\n",
    "\n",
    "1. Define the model and its parameters.\n",
    "2. Choose an appropriate cost function.\n",
    "3. Calculate the negative gradient of the cost function with respect to each parameter.\n",
    "4. Update the parameters using the negative gradient, usually with a learning rate that determines the step size.\n",
    "5. Repeat steps 3 and 4 until the algorithm converges to a local minimum of the cost function.\n",
    "\n",
    "The use of gradient descent in machine learning is essential for optimizing the model's parameters, which is a crucial step in training most machine learning models. By systematically adjusting the parameters to minimize the cost function, the model's performance can be significantly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71ffb6",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a874abf",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical method that models the relationship between two or more independent variables (predictors) and a response (outcome) variable. This relationship is modeled using a linear equation.\n",
    "\n",
    "On the other hand, simple linear regression involves a single independent variable (predictor) and a single response (outcome) variable. It is a simpler model compared to multiple linear regression, as it does not account for the effects of multiple predictors.\n",
    "\n",
    "In terms of implementation, multiple linear regression requires a similar process to simple linear regression, which involves estimating the parameters of the linear model, assessing the goodness of fit, and evaluating the model's performance.\n",
    "\n",
    "Implementing multiple linear regression in Python using the `sklearn.linear_model` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume we have a dataset 'df' with columns 'A', 'B', and 'C' as predictors and 'D' as the response variable\n",
    "X = df[['A', 'B', 'C']]\n",
    "y = df['D']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a4698",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f028cce",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation where two or more predictor variables in a multiple linear regression model are highly correlated with each other. In other words, when two or more predictors in a multiple linear regression model explain a significant portion of the variability of the response variable, the predictors are said to be multicollinear.\n",
    "\n",
    "Detecting multicollinearity can be done using a correlation matrix or variance inflation factor (VIF). A correlation matrix displays the pairwise correlation coefficients between all variables in the model. High positive or negative values indicate high correlation, which suggests multicollinearity. On the other hand, a high VIF indicates high multicollinearity. The VIF for a predictor is defined as the ratio of the variance of the predicted values to the variance of the predicted values with that predictor omitted.\n",
    "\n",
    "Addressing multicollinearity involves one or more of the following strategies:\n",
    "\n",
    "1. Removing one of the correlated predictors: This can be done if one of the predictors provides little additional information to the model.\n",
    "\n",
    "2. Using principal component analysis (PCA): PCA can be used to combine the correlated predictors into a single principal component, thereby reducing multicollinearity.\n",
    "\n",
    "3. Ridge regression: This regularization technique can be used to address multicollinearity by adding a penalty term to the regression coefficients. The penalty term discourages large coefficients, effectively shrinking them towards zero and reducing multicollinearity.\n",
    "\n",
    "4. Lasso regression: Like ridge regression, lasso regression is a regularization technique that can be used to address multicollinearity. However, instead of adding a penalty term to the coefficients, lasso regression adds a penalty term to the absolute value of the coefficients. This can lead to some coefficients being shrunk to zero, effectively eliminating certain predictors from the model.\n",
    "\n",
    "By addressing multicollinearity, you can improve the interpretability of your multiple linear regression model and potentially improve its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159be15b",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3d263",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that models the relationship between a dependent variable and one or more independent variables. However, it introduces non-linear terms, allowing the relationship between the variables to be modeled as a polynomial function.\n",
    "\n",
    "In polynomial regression, each predictor variable can be transformed into a new variable using a monomial (polynomial) function of one or more of its original variables. This creates a higher-degree polynomial, allowing the regression function to capture more complex patterns in the data.\n",
    "\n",
    "For example, a simple linear regression model might be:\n",
    "\n",
    "Y = β0 + β1 * X1 + ε\n",
    "\n",
    "A quadratic polynomial regression model would involve creating new variables that are functions of the original variables. For instance, X1, X2, and X3, where X3 = X1^2, might be used to model a relationship with Y.\n",
    "\n",
    "In this case, the regression model would be:\n",
    "\n",
    "Y = β0 + β1 * X1 + β2 * X2 + β3 * X3 + ε\n",
    "\n",
    "This would be a non-linear relationship between Y and the predictor variables.\n",
    "\n",
    "Polynomial regression is used when a more flexible non-linear model is desired, or when there is reason to believe that the relationship between the variables is non-linear.\n",
    "\n",
    "On the other hand, linear regression assumes a linear relationship between the predictor variables and the response variable. If the true relationship is non-linear, using linear regression can result in a poor fit of the data, leading to inaccurate predictions.\n",
    "\n",
    "Polynomial regression is a technique used to model complex, non-linear relationships between variables. It can be particularly useful when a more flexible model is required or when the true relationship between the variables is non-linear. However, it should be used with caution, as overfitting can occur if the model's complexity is too high for the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25cb91",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a93b5",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Increased Model Complexity: Polynomial regression allows for a more complex relationship between the variables. This can be useful when a more flexible model is needed or when the true relationship between the variables is non-linear.\n",
    "\n",
    "Improved Model Fit: Polynomial regression can result in a better fit of the data compared to linear regression. This can be particularly useful when there is a complex, non-linear relationship between the variables.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: An overly complex polynomial regression model can lead to overfitting. Overfitting occurs when the model becomes too specialized to the training data, resulting in poor performance on new, unseen data.\n",
    "\n",
    "Model Interpretability: Polynomial regression models can be more difficult to interpret than linear regression models. This is because the relationship between the variables in a polynomial regression model is not as straightforward to understand as in a linear regression model.\n",
    "\n",
    "Computational Complexity: Fitting a polynomial regression model can be computationally intensive, especially for higher-degree polynomials. This can slow down the training process and make it more challenging to implement in real-world applications.\n",
    "\n",
    "Influence of Outliers: Polynomial regression models can be more sensitive to outliers in the data than linear regression models. This is because the shape of the polynomial regression curve can be significantly influenced by the presence of outliers.\n",
    "\n",
    "In situations where the true relationship between the variables is complex and non-linear, polynomial regression can be a more suitable choice. However, it is important to consider factors such as model complexity, computational resources, and potential impact on model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989cadfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
