{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8858bf9b-7fac-4912-9887-e682d073879c",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "**Decision Tree Classifier Algorithm:**\n",
    "- **Overview:** A decision tree classifier is a tree-structured model used to make predictions by recursively splitting the data into subsets based on feature values.\n",
    "- **Working:**\n",
    "  1. **Start at the Root:** Begin with the entire dataset at the root of the tree.\n",
    "  2. **Select Best Feature:** Choose the feature that best splits the data based on a criterion like Gini impurity or Information Gain.\n",
    "  3. **Create Branches:** Split the dataset into subsets, each corresponding to a value or range of the chosen feature.\n",
    "  4. **Repeat Recursively:** Repeat the process for each subset, creating branches and nodes until a stopping criterion is met (e.g., all instances in a node belong to the same class, or a maximum tree depth is reached).\n",
    "  5. **Make Predictions:** For a given input, traverse the tree from the root to a leaf node by following the branches corresponding to the input's feature values. The leaf node provides the predicted class.\n",
    "\n",
    "## 2\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "1. **Impurity Measures:** Used to select the best feature for splitting.\n",
    "   - **Gini Impurity:**\n",
    "     \\[\n",
    "     Gini(D) = 1 - ∑{i=1}^C p_i^2\n",
    "     \\]\n",
    "     \n",
    "     where \\(p_i\\) is the probability of class \\(i\\) in dataset \\(D\\).\n",
    "   - **Information Gain:**\n",
    "     \\[\n",
    "     IG(D, A) = Entropy(D) - ∑{v∈Values(A)} (|D_v|/|D|) Entropy(D_v)\n",
    "     \\]\n",
    "     \n",
    "     where \\(D_v\\) is the subset of \\(D\\) where feature \\(A\\) has value \\(v\\).\n",
    "\n",
    "2. **Entropy:**\n",
    "   \\[\n",
    "   Entropy(D) = - ∑{i=1}^C p_i . log_2(p_i)\n",
    "   \\]\n",
    "\n",
    "3. **Feature Selection:** Choose the feature that minimizes impurity or maximizes information gain.\n",
    "\n",
    "4. **Splitting:** Divide the dataset based on the selected feature and repeat the process for each subset.\n",
    "\n",
    "5. **Stopping Criteria:** Stop when all instances in a node are of the same class, or the maximum tree depth is reached.\n",
    "\n",
    "## 3\n",
    "\n",
    "**Using Decision Tree for Binary Classification:**\n",
    "- **Binary Classification:** The task is to classify instances into one of two classes.\n",
    "- **Process:**\n",
    "  1. **Root Node:** Start with the entire dataset.\n",
    "  2. **Feature Selection:** Select the best feature that splits the data into subsets to reduce impurity.\n",
    "  3. **Splitting:** Split the data into two subsets based on the feature value (e.g., yes/no, true/false).\n",
    "  4. **Recursive Splitting:** Apply the process recursively to each subset.\n",
    "  5. **Leaf Nodes:** Each leaf node represents a predicted class (0 or 1).\n",
    "- **Prediction:** For a new instance, traverse the tree from the root to a leaf node using the instance's feature values. The leaf node gives the predicted class.\n",
    "\n",
    "## 4\n",
    "\n",
    "**Geometric Intuition:**\n",
    "- **Decision Boundaries:** Decision trees create axis-aligned decision boundaries.\n",
    "- **Splitting Space:** Each split divides the feature space into rectangular regions.\n",
    "- **Leaf Nodes:** Each leaf node corresponds to a region in the feature space, with instances in the same region predicted to be of the same class.\n",
    "\n",
    "**Predictions:**\n",
    "- **Traversal:** For a given input, start at the root and follow the splits based on feature values.\n",
    "- **Region:** The input falls into one of the rectangular regions.\n",
    "- **Class:** The class associated with the corresponding leaf node is the predicted class.\n",
    "\n",
    "## 5\n",
    "\n",
    "**Confusion Matrix:**\n",
    "- **Definition:** A table that summarizes the performance of a classification model by comparing actual and predicted classes.\n",
    "- **Structure:**\n",
    "  \\[\n",
    "  \\begin{array}{|c|c|c|}\n",
    "  \\hline\n",
    "  & \\text{Predicted Positive} & \\text{Predicted Negative} \\\\\n",
    "  \\hline\n",
    "  \\text{Actual Positive} & TP & FN \\\\\n",
    "  \\hline\n",
    "  \\text{Actual Negative} & FP & TN \\\\\n",
    "  \\hline\n",
    "  \\end{array}\n",
    "  \\]\n",
    "\n",
    "**Use:**\n",
    "- **True Positives (TP):** Correctly predicted positive instances.\n",
    "- **True Negatives (TN):** Correctly predicted negative instances.\n",
    "- **False Positives (FP):** Incorrectly predicted positive instances.\n",
    "- **False Negatives (FN):** Incorrectly predicted negative instances.\n",
    "\n",
    "## 6\n",
    "\n",
    "**Example Confusion Matrix:**\n",
    "\\[\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& \\text{Predicted Positive} & \\text{Predicted Negative} \\\\\n",
    "\\hline\n",
    "\\text{Actual Positive} & 50 & 10 \\\\\n",
    "\\hline\n",
    "\\text{Actual Negative} & 5 & 35 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\]\n",
    "\n",
    "**Calculations:**\n",
    "- **Precision:**\n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{50}{50 + 5} = 0.91\n",
    "  \\]\n",
    "\n",
    "- **Recall:**\n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{50}{50 + 10} = 0.83\n",
    "  \\]\n",
    "\n",
    "- **F1 Score:**\n",
    "  \\[\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{0.91 \\cdot 0.83}{0.91 + 0.83} = 0.87\n",
    "  \\]\n",
    "\n",
    "## 7\n",
    "\n",
    "**Importance of Choosing an Appropriate Metric:**\n",
    "- **Context-Specific:** Different problems have different costs associated with false positives and false negatives.\n",
    "- **Imbalanced Data:** Accuracy might be misleading; precision, recall, and F1 score can provide better insights.\n",
    "\n",
    "**How to Choose:**\n",
    "- **Understand the Problem:** Consider the impact of different types of errors.\n",
    "- **Data Distribution:** Analyze the class distribution.\n",
    "- **Business Goals:** Align the metric with business objectives.\n",
    "\n",
    "## 8\n",
    "\n",
    "**Example: Spam Detection**\n",
    "- **Importance of Precision:** In spam detection, high precision is important to minimize the number of legitimate emails marked as spam (false positives).\n",
    "- **Impact:** False positives can lead to important emails being missed, which can be costly for users.\n",
    "\n",
    "## 9\n",
    "\n",
    "**Example: Medical Diagnosis**\n",
    "- **Importance of Recall:** In medical diagnosis, high recall is crucial to ensure that all potential cases of a disease are identified (minimizing false negatives).\n",
    "- **Impact:** Missing a positive case (false negative) can lead to a failure to provide necessary treatment, which can have severe consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1975c0-bb68-42f6-b529-aa0025b81ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
