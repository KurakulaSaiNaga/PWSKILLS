{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a518f5-ec0f-4c48-8e7c-e56909b5e1c3",
   "metadata": {},
   "source": [
    "\n",
    "## 1\n",
    "\n",
    "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds a penalty to the OLS (Ordinary Least Squares) objective function, proportional to the absolute values of the coefficients.\n",
    "\n",
    "**Differences:**\n",
    "- **Regularization:** Lasso regression uses  L1  regularization (absolute values of coefficients) compared to Ridge regression which uses L2  regularization (squared values of coefficients).\n",
    "- **Feature Selection:** Lasso can shrink coefficients to zero, effectively performing feature selection, whereas Ridge regression only shrinks coefficients towards zero but not to zero.\n",
    "\n",
    "## 2\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is its ability to automatically select a subset of relevant features by setting some coefficients to exactly zero. This results in a simpler model that includes only the most influential predictors, which can improve interpretability and reduce overfitting.\n",
    "\n",
    "## 3\n",
    "\n",
    "Interpreting Lasso Regression coefficients involves considering the following:\n",
    "- **Magnitude:** The magnitude indicates the strength of the relationship between each predictor and the response variable.\n",
    "- **Sign:** The sign (+/-) indicates the direction of the relationship.\n",
    "- **Zero Coefficients:** Coefficients that are zero indicate that the corresponding predictor has been excluded from the model, implying it has little to no influence on the response variable.\n",
    "\n",
    "## 4\n",
    "\n",
    "The main tuning parameter in Lasso Regression is ùúÜ, which controls the strength of regularization:\n",
    "- **ùúÜ:** Increasing ùúÜ increases the penalty on the absolute size of the coefficients, leading to more coefficients being set to zero (more sparsity) and potentially reducing overfitting.\n",
    "- **Performance Impact:** Choosing an appropriate ùúÜ balances model complexity (number of selected features) and predictive accuracy. Cross-validation is often used to find the optimal ùúÜ value.\n",
    "\n",
    "## 5\n",
    "\n",
    "Lasso Regression itself is inherently a linear regression technique. However, it can be extended for non-linear regression problems by incorporating non-linear transformations of the predictors (e.g., polynomial features) before applying Lasso Regression. Alternatively, kernel methods can be used to map the data into a higher-dimensional space where linear methods like Lasso can be applied effectively.\n",
    "\n",
    "## 6\n",
    "\n",
    "**Ridge Regression:**\n",
    "- Uses L2 regularization (penalty on squared coefficients).\n",
    "- Can shrink coefficients towards zero but not exactly to zero.\n",
    "- Suitable for reducing multicollinearity and stabilizing coefficient estimates.\n",
    "\n",
    "**Lasso Regression:**\n",
    "- Uses L1 regularization (penalty on absolute coefficients).\n",
    "- Can shrink coefficients to exactly zero, performing feature selection.\n",
    "- Suitable for feature selection and producing sparse models.\n",
    "\n",
    "## 7\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity by shrinking the coefficients of correlated variables towards zero. This helps in reducing the impact of multicollinearity and selecting one of the correlated variables (or setting both to zero if neither is sufficiently important).\n",
    "\n",
    "## 8\n",
    "\n",
    "Choosing the optimal ùúÜ (regularization parameter) in Lasso Regression typically involves:\n",
    "- **Cross-Validation:** Use techniques like k-fold cross-validation to evaluate the model performance for different values of ùúÜ.\n",
    "- **Grid Search:** Implement a grid search over a range of ùúÜ values to find the one that minimizes prediction error or maximizes a chosen performance metric (e.g.,  R^2  score).\n",
    "- **Information Criteria:** Utilize information criteria such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to balance model fit and complexity.\n",
    "\n",
    "By systematically testing different values of ùúÜ and evaluating model performance, you can determine the optimal regularization parameter for your Lasso Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f1b64-33f9-47da-94cf-6427e4a6c32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
