{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cd1a54",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "The k-nearest neighbors (KNN) algorithm is a simple and intuitive supervised machine learning algorithm used for classification and regression tasks. It works based on the principle that similar data points are close to each other in a feature space. In KNN, to classify a new data point, the algorithm finds the k nearest data points in the training set (nearest neighbors) and assigns the majority class label (for classification) or averages their values (for regression) to the new data point.\n",
    "\n",
    "## 2\n",
    "\n",
    "The choice of the value of K in KNN is crucial as it directly affects the performance of the algorithm. A smaller value of K leads to more complex decision boundaries, which can result in overfitting, while a larger value of K leads to smoother decision boundaries but may increase bias. The value of K is typically chosen using techniques such as cross-validation or by trying different values and evaluating performance metrics.\n",
    "\n",
    "## 3\n",
    "\n",
    "- KNN Classifier: Used for classification tasks, where the output is a class label. It assigns the class label based on the majority class of the k-nearest neighbors.\n",
    "- KNN Regressor: Used for regression tasks, where the output is a continuous value. It predicts the output value based on the average of the values of the k-nearest neighbors.\n",
    "\n",
    "## 4\n",
    "\n",
    "The performance of KNN can be measured using various evaluation metrics depending on the task:\n",
    "\n",
    "- For classification: Accuracy, precision, recall, F1-score, ROC curve, etc.\n",
    "- For regression: Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared (coefficient of determination), etc.\n",
    "\n",
    "## 5\n",
    "\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of dimensions (features) in the data increases. In high-dimensional spaces, the concept of distance becomes less meaningful because data points tend to become equidistant from each other, making it difficult for KNN to discern meaningful patterns.\n",
    "\n",
    "## 6\n",
    "\n",
    "One approach to handle missing values in KNN is to impute them with a value computed based on the k-nearest neighbors of the data point with missing values. Alternatively, you can use techniques like mean or median imputation before applying KNN.\n",
    "\n",
    "## 7\n",
    "\n",
    "- KNN Classifier: Suitable for classification tasks where the output is categorical (class labels). It works well when the decision boundaries are not highly nonlinear and the dataset is not too large.\n",
    "- KNN Regressor: Suitable for regression tasks where the output is continuous. It works well when the data has a smooth underlying structure and the number of dimensions is not too high.\n",
    "\n",
    "## 8\n",
    "\n",
    "Strengths:\n",
    "- Simple and easy to implement.\n",
    "- No training phase, making it suitable for online learning.\n",
    "- Non-parametric nature allows it to handle complex decision boundaries.\n",
    "\n",
    "Weaknesses:\n",
    "- Computationally expensive during prediction, especially for large datasets.\n",
    "- Sensitive to the choice of K and distance metric.\n",
    "- Performance deteriorates in high-dimensional spaces (curse of dimensionality).\n",
    "\n",
    "To address these weaknesses, techniques such as dimensionality reduction, distance metric optimization, and efficient data structures (e.g., KD-trees) can be used.\n",
    "\n",
    "## 9\n",
    "\n",
    "- Euclidean Distance: Measures the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between corresponding coordinates.\n",
    "- Manhattan Distance: Also known as city block distance or L1 norm, measures the distance between two points by summing the absolute differences of their coordinates. It is calculated as the sum of the absolute differences of the coordinates.\n",
    "\n",
    "## 10\n",
    "\n",
    "Feature scaling is important in KNN because it ensures that all features contribute equally to the distance calculation. Since KNN relies on the distance between data points, features with larger scales may dominate the distance calculation, leading to biased results. Common techniques for feature scaling include normalization (scaling features to a range) and standardization (scaling features to have zero mean and unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c96d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
