{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ccc7e7",
   "metadata": {},
   "source": [
    "## 1\n",
    "An activation function in the context of artificial neural networks is a mathematical operation applied to the output of each neuron in a neural network. It introduces non-linearity to the network, allowing it to learn complex patterns and relationships in the data.\n",
    "\n",
    "## 2\n",
    "Common types of activation functions used in neural networks include:\n",
    "   - Sigmoid\n",
    "   - Hyperbolic tangent (tanh)\n",
    "   - Rectified Linear Unit (ReLU)\n",
    "   - Leaky ReLU\n",
    "   - Parametric ReLU (PReLU)\n",
    "   - Exponential Linear Unit (ELU)\n",
    "   - Softmax\n",
    "\n",
    "## 3\n",
    "Activation functions affect the training process and performance of a neural network by introducing non-linearity, enabling the network to learn and model complex relationships in the data. They also play a crucial role in controlling the output range of neurons, preventing them from saturating or becoming too large.\n",
    "\n",
    "## 4\n",
    "The sigmoid activation function squashes input values to a range between 0 and 1. It is commonly used in the output layer of binary classification models. Advantages include smooth gradients, and the output can be interpreted as probabilities. Disadvantages include the vanishing gradient problem and the fact that the output is not zero-centered.\n",
    "\n",
    "## 5\n",
    "The Rectified Linear Unit (ReLU) activation function replaces all negative input values with zero and passes positive values unchanged. It is popular due to its simplicity and efficiency. Unlike the sigmoid function, ReLU does not saturate for positive inputs.\n",
    "\n",
    "## 6\n",
    "The benefits of using the ReLU activation function over the sigmoid function include faster convergence during training, reduced likelihood of the vanishing gradient problem, and simplicity in implementation.\n",
    "\n",
    "## 7\n",
    "Leaky ReLU is a variant of the ReLU activation function that allows a small, positive slope for negative inputs, preventing neurons from becoming completely inactive. It addresses the vanishing gradient problem by providing a non-zero gradient for negative inputs.\n",
    "\n",
    "## 8\n",
    "The softmax activation function is used in the output layer of a neural network for multiclass classification problems. It converts raw output scores into probability distributions, allowing the model to predict the probability of each class.\n",
    "\n",
    "## 9\n",
    "The hyperbolic tangent (tanh) activation function is similar to the sigmoid but squashes input values to a range between -1 and 1. It is zero-centered, which helps mitigate the vanishing gradient problem. It is often used in the hidden layers of neural networks and is an improvement over the sigmoid function in terms of zero-centered outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408361c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
