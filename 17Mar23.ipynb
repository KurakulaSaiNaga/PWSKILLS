{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b51652",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69c0e4",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of values for certain observations or variables. These missing values can occur for various reasons, such as data entry errors, sensor malfunctions, or intentional non-responses by survey participants. Handling missing values is essential for several reasons:\n",
    "\n",
    "1. **Biased Analysis:** Ignoring missing values can lead to biased or inaccurate results in data analysis, as the available data may not be representative of the entire dataset.\n",
    "\n",
    "2. **Reduced Statistical Power:** Missing values can reduce the statistical power of a study, making it more challenging to detect meaningful patterns or relationships in the data.\n",
    "\n",
    "3. **Model Performance:** Many machine learning algorithms and statistical models cannot handle missing values, leading to errors or suboptimal performance if not addressed.\n",
    "\n",
    "4. **Data Quality:** Handling missing values improves the overall quality of the dataset, making it more reliable and suitable for analysis.\n",
    "\n",
    "Several methods can be employed to handle missing values, including imputation (replacing missing values with estimated values) or removing observations or variables with missing values. However, the choice of method depends on the nature of the data and the specific analysis goals.\n",
    "\n",
    "While some algorithms can handle missing values inherently or with minimal preprocessing, others may require more effort in handling missing data. Algorithms that are generally not affected by missing values or are more robust include:\n",
    "\n",
    "1. **Decision Trees:** Decision tree-based algorithms, such as Random Forests and Gradient Boosted Trees, can handle missing values without requiring imputation. They naturally account for missing data during the tree-building process.\n",
    "\n",
    "2. **k-Nearest Neighbors (k-NN):** k-NN is a non-parametric algorithm that can be robust to missing values. The imputation can be performed by considering the values from the nearest neighbors.\n",
    "\n",
    "3. **Support Vector Machines (SVM):** SVMs can handle missing data by excluding missing values from the decision function's computation.\n",
    "\n",
    "4. **Naive Bayes:** Naive Bayes classifiers can work with missing values, although imputation may be necessary for optimal performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e318ab9",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac066d",
   "metadata": {},
   "source": [
    "Handling missing data is crucial for accurate and reliable analysis.\n",
    "Common techniques for handling missing data along with examples using Python:\n",
    "\n",
    "1. **Deletion of Missing Data:**\n",
    "   - Remove rows or columns with missing values.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_no_missing_rows = df.dropna(axis=0)\n",
    "\n",
    "df_no_missing_cols = df.dropna(axis=1)\n",
    "\n",
    "print(\"DataFrame with no missing rows:\\n\", df_no_missing_rows)\n",
    "print(\"\\nDataFrame with no missing columns:\\n\", df_no_missing_cols)\n",
    "```\n",
    "\n",
    "2. **Mean/Median/Mode Imputation:**\n",
    "   - Fill missing values with the mean, median, or mode of the respective column.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "df_imputed_mean = df.fillna(df.mean())\n",
    "\n",
    "print(\"DataFrame after mean imputation:\\n\", df_imputed_mean)\n",
    "```\n",
    "\n",
    "3. **Forward Fill (ffill) or Backward Fill (bfill):**\n",
    "   - Propagate non-missing values forward or backward to fill missing values.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, None, 5], 'B': [5, None, 7, None, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_forward_fill = df.ffill()\n",
    "\n",
    "df_backward_fill = df.bfill()\n",
    "\n",
    "print(\"DataFrame after forward fill:\\n\", df_forward_fill)\n",
    "print(\"\\nDataFrame after backward fill:\\n\", df_backward_fill)\n",
    "```\n",
    "\n",
    "4. **Interpolation:**\n",
    "   - Interpolate missing values based on existing values in the column.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, None, 5], 'B': [5, None, 7, None, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolated = df.interpolate()\n",
    "\n",
    "print(\"DataFrame after interpolation:\\n\", df_interpolated)\n",
    "```\n",
    "\n",
    "5. **Imputation Using Machine Learning Models:**\n",
    "   - Use machine learning models to predict missing values based on other features.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "columns_with_missing = df.columns[df.isnull().any()].tolist()\n",
    "\n",
    "# Impute missing values using Random Forest regression\n",
    "for column in columns_with_missing:\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[column] = imputer.fit_transform(df[[column]])\n",
    "\n",
    "print(\"DataFrame after machine learning imputation:\\n\", df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d8e70",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937e4e0",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in a classification problem where the distribution of classes is not uniform, meaning that one class has significantly fewer instances than the others. In a binary classification scenario, it often manifests as a large disparity in the number of examples between the two classes. For example, in a fraud detection task, the majority of transactions may be legitimate, with only a small percentage being fraudulent.\n",
    "\n",
    "Key characteristics of imbalanced data:\n",
    "\n",
    "1. **Skewed Class Distribution:** One class (the minority class) has much fewer instances compared to the other class (the majority class).\n",
    "\n",
    "2. **Challenges in Model Training:** Machine learning models trained on imbalanced data may be biased towards the majority class, as they tend to focus on accuracy, which can be misleading in imbalanced scenarios.\n",
    "\n",
    "3. **Difficulty in Learning Minority Class Patterns:** The model may have difficulty learning patterns from the minority class due to the limited number of examples, leading to poor generalization on unseen minority class instances.\n",
    "\n",
    "4. **Evaluation Metrics Misleading:** Standard classification accuracy may not be a reliable metric for assessing model performance, as a model could achieve high accuracy by simply predicting the majority class for all instances.\n",
    "\n",
    "If imbalanced data is not handled, several issues may arise:\n",
    "\n",
    "1. **Bias Towards the Majority Class:** Models trained on imbalanced data may show a bias towards predicting the majority class. This is especially true for algorithms that optimize for overall accuracy.\n",
    "\n",
    "2. **Poor Generalization to Minority Class:** The model may perform poorly on instances from the minority class, as it has not been exposed to enough examples to learn their patterns effectively.\n",
    "\n",
    "3. **Misleading Evaluation Metrics:** Accuracy can be misleading, as a model might achieve high accuracy by predicting the majority class, even if it fails to correctly classify minority class instances.\n",
    "\n",
    "To address imbalanced data, various techniques can be employed, including:\n",
    "\n",
    "1. **Resampling:** This involves either oversampling the minority class, undersampling the majority class, or a combination of both.\n",
    "\n",
    "2. **Synthetic Data Generation:** Techniques such as SMOTE (Synthetic Minority Over-sampling Technique) create synthetic instances of the minority class to balance the class distribution.\n",
    "\n",
    "3. **Cost-sensitive Learning:** Assigning different misclassification costs to different classes can guide the model to pay more attention to the minority class.\n",
    "\n",
    "4. **Ensemble Methods:** Using ensemble methods, such as Random Forests or boosting algorithms, can improve performance on imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ec8b7",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cb12e",
   "metadata": {},
   "source": [
    "**Up-sampling and down-sampling** are techniques used to address imbalanced datasets by adjusting the class distribution, either by increasing the number of instances in the minority class (up-sampling) or decreasing the number of instances in the majority class (down-sampling).\n",
    "\n",
    "### Up-sampling:\n",
    "\n",
    "**Definition:**\n",
    "Up-sampling involves increasing the number of instances in the minority class by randomly duplicating existing instances or generating synthetic examples.\n",
    "\n",
    "**Example Scenario:**\n",
    "Let's consider a credit card fraud detection dataset where only 1% of transactions are fraudulent (minority class). The model trained on this dataset may struggle to identify patterns related to fraudulent transactions due to the small number of examples. In this case, up-sampling can be applied by creating additional instances of fraudulent transactions, either by duplicating existing ones or by generating synthetic examples using techniques like SMOTE.\n",
    "\n",
    "**Python Example:**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.99, 0.01],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE for up-sampling\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "```\n",
    "\n",
    "### Down-sampling:\n",
    "\n",
    "**Definition:**\n",
    "Down-sampling involves reducing the number of instances in the majority class by randomly removing instances or using sampling techniques.\n",
    "\n",
    "**Example Scenario:**\n",
    "Continuing with the credit card fraud detection example, if the majority class (legitimate transactions) has a large number of instances, down-sampling can be applied to create a more balanced dataset. This can involve randomly removing instances from the majority class.\n",
    "\n",
    "**Python Example:**\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.99, 0.01],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply random under-sampling\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "```\n",
    "\n",
    "### When to Use Up-sampling and Down-sampling:\n",
    "\n",
    "- **Up-sampling:** Use when the minority class is underrepresented, and generating synthetic examples or duplicating existing instances can help the model learn its patterns better.\n",
    "\n",
    "- **Down-sampling:** Use when the majority class is significantly larger, and you want to create a more balanced dataset by removing instances from the majority class.\n",
    "\n",
    "The choice between up-sampling and down-sampling depends on the characteristics of the dataset and the specific goals of the analysis or modeling task. Sometimes, a combination of both techniques (hybrid methods) may be used for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3cc5be",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f3366",
   "metadata": {},
   "source": [
    "**Data augmentation** is a technique used to increase the diversity of a dataset by applying various transformations to the existing data, creating additional examples. This is commonly used in machine learning, particularly in scenarios where the dataset is limited or imbalanced. By augmenting the data, the model can be exposed to a more comprehensive range of variations, improving its ability to generalize to new, unseen data.\n",
    "\n",
    "One specific data augmentation technique, often used in the context of imbalanced datasets, is **Synthetic Minority Over-sampling Technique (SMOTE)**.\n",
    "\n",
    "### SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "**Definition:**\n",
    "SMOTE is an algorithm that aims to balance class distribution in a dataset by generating synthetic examples for the minority class. It works by creating synthetic instances that are combinations of existing minority class instances, effectively expanding the minority class and making it more proportionate to the majority class.\n",
    "\n",
    "**How SMOTE Works:**\n",
    "1. **Select a Minority Instance:** For each minority instance in the dataset, SMOTE selects a neighbor from its k-nearest neighbors. The value of k is a parameter chosen by the user.\n",
    "\n",
    "2. **Generate Synthetic Instance:** A synthetic instance is created by interpolating between the selected minority instance and its chosen neighbor. This is done by selecting a random value between 0 and 1 for each feature and computing a weighted sum of the two instances.\n",
    "\n",
    "3. **Repeat:** Steps 1 and 2 are repeated until the desired balance between the minority and majority classes is achieved.\n",
    "\n",
    "**Python Example using SMOTE:**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.99, 0.01],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE for over-sampling\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "```\n",
    "\n",
    "In this example, SMOTE is applied to the training set (`X_train` and `y_train`). It generates synthetic instances for the minority class, making the distribution more balanced.\n",
    "\n",
    "SMOTE helps prevent the model from being biased toward the majority class and can improve the model's performance, especially when dealing with imbalanced datasets. It is important to note that while SMOTE is a powerful technique, it may not always be suitable for every dataset, and its effectiveness depends on the characteristics of the data and the specific modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa09818",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daed6e",
   "metadata": {},
   "source": [
    "**Outliers** in a dataset are data points that significantly differ from the majority of other data points. These are observations that deviate markedly from the overall pattern of the data. Outliers can be present in one or more dimensions (features) and can skew statistical analyses, leading to misleading interpretations and impacting the performance of machine learning models. Outliers can arise due to various reasons, such as errors in data collection, measurement variability, or the presence of rare events.\n",
    "\n",
    "**Why it is essential to handle outliers:**\n",
    "\n",
    "1. **Impact on Descriptive Statistics:** Outliers can heavily influence summary statistics like the mean and standard deviation. The mean, in particular, is sensitive to extreme values, and its accuracy may be compromised if outliers are present.\n",
    "\n",
    "2. **Distorted Data Distributions:** Outliers can distort the shape of data distributions, making them appear skewed or non-normally distributed. This can affect the validity of statistical tests and assumptions.\n",
    "\n",
    "3. **Model Performance:** Outliers can adversely affect the performance of machine learning models. Some models, especially those based on distance metrics (e.g., k-Nearest Neighbors), can be sensitive to outliers and produce suboptimal results.\n",
    "\n",
    "4. **Regression Analysis:** Outliers can disproportionately impact regression models by influencing the slope and intercept. This can lead to inaccurate predictions and reduced model interpretability.\n",
    "\n",
    "5. **Robustness of Models:** Outliers can compromise the robustness of statistical and machine learning models, potentially leading to overfitting or underfitting.\n",
    "\n",
    "6. **Data Understanding and Interpretation:** Outliers can distort the interpretation of the underlying patterns in the data. Handling outliers is crucial for obtaining a more accurate understanding of the data and drawing meaningful conclusions.\n",
    "\n",
    "**Common methods for handling outliers:**\n",
    "\n",
    "1. **Identifying and Removing Outliers:** Use statistical methods (e.g., Z-score, IQR) to identify outliers and then remove or modify them. This approach is suitable when outliers are likely to be errors or anomalies.\n",
    "\n",
    "2. **Transformations:** Apply mathematical transformations (e.g., log transformation) to make the data less sensitive to outliers and improve its normality.\n",
    "\n",
    "3. **Winsorizing:** Replace extreme values with less extreme, but still plausible, values. This helps mitigate the impact of outliers without entirely removing them.\n",
    "\n",
    "4. **Imputation:** Impute missing values or outliers using statistical methods or predictive models. This approach is particularly useful when outliers are not errors but represent valid extreme observations.\n",
    "\n",
    "5. **Model Robustness:** Choose models that are less sensitive to outliers. For example, robust regression techniques or models based on tree ensembles (e.g., Random Forests) are less affected by outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2cea3",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb212a",
   "metadata": {},
   "source": [
    "Handling missing data is a crucial step in the data analysis process to ensure accurate and meaningful results.\n",
    "Several techniques we can use to handle missing data in your analysis:\n",
    "\n",
    "1. **Data Imputation:**\n",
    "   - **Mean, Median, or Mode Imputation:** Replace missing values with the mean, median, or mode of the respective column.\n",
    "     ```python\n",
    "     import pandas as pd\n",
    "\n",
    "     # Assuming 'df' is your DataFrame\n",
    "     df_filled_mean = df.fillna(df.mean())\n",
    "     ```\n",
    "\n",
    "   - **Forward Fill (ffill) or Backward Fill (bfill):** Propagate non-missing values forward or backward.\n",
    "     ```python\n",
    "     df_filled_forward = df.ffill()\n",
    "     ```\n",
    "\n",
    "   - **Interpolation:** Estimate missing values based on the values of other data points.\n",
    "     ```python\n",
    "     df_filled_interpolated = df.interpolate()\n",
    "     ```\n",
    "\n",
    "2. **Deletion of Missing Data:**\n",
    "   - **Listwise Deletion:** Remove entire rows with missing values.\n",
    "     ```python\n",
    "     df_no_missing_rows = df.dropna(axis=0)\n",
    "     ```\n",
    "\n",
    "   - **Column-wise Deletion:** Remove columns with a significant number of missing values.\n",
    "     ```python\n",
    "     df_no_missing_cols = df.dropna(axis=1)\n",
    "     ```\n",
    "\n",
    "3. **Imputation Using Machine Learning Models:**\n",
    "   - **K-Nearest Neighbors (KNN) Imputation:** Predict missing values based on the values of their k-nearest neighbors.\n",
    "     ```python\n",
    "     from sklearn.impute import KNNImputer\n",
    "\n",
    "     imputer = KNNImputer(n_neighbors=2)\n",
    "     df_imputed_knn = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "     ```\n",
    "\n",
    "   - **Random Forest Imputation:** Use a Random Forest model to predict missing values.\n",
    "     ```python\n",
    "     from sklearn.impute import SimpleImputer\n",
    "     from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "     imputer = SimpleImputer(strategy='mean')\n",
    "     df_imputed_rf = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "     ```\n",
    "\n",
    "4. **Handling Categorical Data:**\n",
    "   - **Mode Imputation for Categorical Data:** Replace missing categorical values with the mode (most frequent category).\n",
    "     ```python\n",
    "     df_categorical_imputed = df_categorical.fillna(df_categorical.mode().iloc[0])\n",
    "     ```\n",
    "\n",
    "   - **Label Encoding or One-Hot Encoding:** Convert categorical data into a numerical format for use in machine learning models.\n",
    "     ```python\n",
    "     df_encoded = pd.get_dummies(df_categorical, drop_first=True)\n",
    "     ```\n",
    "\n",
    "5. **Advanced Imputation Techniques:**\n",
    "   - **Multiple Imputation:** Generate multiple imputations to account for uncertainty in imputed values.\n",
    "     ```python\n",
    "     from sklearn.impute import IterativeImputer\n",
    "\n",
    "     imputer = IterativeImputer()\n",
    "     df_imputed_iterative = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "     ```\n",
    "\n",
    "   - **Expectation-Maximization (EM) Algorithm:** Iteratively estimate missing values based on conditional probabilities.\n",
    "     ```python\n",
    "     from sklearn.impute import SimpleImputer\n",
    "     from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "     imputer = SimpleImputer(strategy='mean')\n",
    "     df_imputed_em = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6751d7c",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e498c",
   "metadata": {},
   "source": [
    "When dealing with missing data in a large dataset, it's essential to understand whether the missingness is completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). \n",
    "Some strategies to assess the missing data patterns:\n",
    "\n",
    "1. **Visual Inspection:**\n",
    "   - **Missing Data Heatmap:** Create a heatmap to visualize the distribution of missing values across variables. This can help identify patterns visually.\n",
    "     ```python\n",
    "     import seaborn as sns\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     plt.figure(figsize=(10, 8))\n",
    "     sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "2. **Missing Data Statistics:**\n",
    "   - **Missingness Summary:** Generate summary statistics to understand the percentage of missing values for each variable.\n",
    "     ```python\n",
    "     missing_data_summary = df.isnull().mean()\n",
    "     ```\n",
    "\n",
    "   - **Missingness Correlation:** Examine correlations between missing values in different variables. A positive correlation might suggest a systematic pattern.\n",
    "     ```python\n",
    "     missing_data_correlation = df.corr()\n",
    "     ```\n",
    "\n",
    "3. **Statistical Tests:**\n",
    "   - **Little's MCAR Test:** Use statistical tests like Little's MCAR test to assess whether the missingness is completely random.\n",
    "     ```python\n",
    "     from missingpy import MissForest\n",
    "     from missingpy import missforest\n",
    "\n",
    "     imputer = MissForest()\n",
    "     df_imputed = imputer.fit_transform(df)\n",
    "     ```\n",
    "\n",
    "4. **Pattern Analysis:**\n",
    "   - **Compare Distributions:** Compare the distributions of variables with and without missing values to identify patterns.\n",
    "     ```python\n",
    "     import seaborn as sns\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     sns.histplot(df['variable_with_missing'], label='With Missing', kde=True)\n",
    "     sns.histplot(df['variable_no_missing'], label='No Missing', kde=True)\n",
    "     plt.legend()\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "   - **Temporal Patterns:** If your data has a temporal dimension, check for patterns over time.\n",
    "\n",
    "5. **Domain Knowledge:**\n",
    "   - **Consult Experts:** Consult domain experts to understand whether there are known reasons for missing data based on the context of the dataset.\n",
    "\n",
    "6. **Machine Learning Models:**\n",
    "   - **Predictive Modeling:** Train machine learning models to predict missing values based on other variables. If the model performs well, it suggests patterns in the missingness.\n",
    "     ```python\n",
    "     from sklearn.model_selection import train_test_split\n",
    "     from sklearn.ensemble import RandomForestRegressor\n",
    "     from sklearn.metrics import mean_squared_error\n",
    "\n",
    "     df_missing = df[df['variable_with_missing'].isnull()]\n",
    "     df_no_missing = df.dropna(subset=['variable_with_missing'])\n",
    "\n",
    "     X_train, X_test, y_train, y_test = train_test_split(df_no_missing.drop(['variable_with_missing'], axis=1),\n",
    "                                                         df_no_missing['variable_with_missing'],\n",
    "                                                         test_size=0.2, random_state=42)\n",
    "\n",
    "     model = RandomForestRegressor()\n",
    "     model.fit(X_train, y_train)\n",
    "\n",
    "     predictions = model.predict(df_missing.drop(['variable_with_missing'], axis=1))\n",
    "\n",
    "     mse = mean_squared_error(df_missing['variable_with_missing'], predictions)\n",
    "     ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1cc7a",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b571467",
   "metadata": {},
   "source": [
    "Dealing with imbalanced datasets, especially in a medical diagnosis project where the positive class (presence of the condition) is rare, requires careful consideration of performance evaluation strategies. \n",
    "Some strategies you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "1. **Use Appropriate Evaluation Metrics:**\n",
    "- **Precision and Recall:** Focus on precision and recall rather than accuracy. Precision measures the accuracy of positive predictions, while recall measures the ability of the model to capture all positive instances.\n",
    "   ```python\n",
    "   from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "   precision = precision_score(y_true, y_pred)\n",
    "   recall = recall_score(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "- **F1 Score:** Compute the F1 score, which is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "   ```python\n",
    "   from sklearn.metrics import f1_score\n",
    "\n",
    "   f1 = f1_score(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "- **Area Under the Precision-Recall Curve (AUC-PR):** Consider using the AUC-PR to evaluate the trade-off between precision and recall across different decision thresholds.\n",
    "   ```python\n",
    "   from sklearn.metrics import precision_recall_curve, auc\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "   auc_pr = auc(recall, precision)\n",
    "\n",
    "   plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.2f}')\n",
    "   plt.xlabel('Recall')\n",
    "   plt.ylabel('Precision')\n",
    "   plt.legend()\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "2. **Confusion Matrix Analysis:**\n",
    "- **Confusion Matrix:** Examine the confusion matrix to understand how many true positive, true negative, false positive, and false negative predictions the model is making.\n",
    "   ```python\n",
    "   from sklearn.metrics import confusion_matrix\n",
    "\n",
    "   conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "- **Normalized Confusion Matrix:** Consider normalizing the confusion matrix to obtain proportions rather than counts.\n",
    "   ```python\n",
    "   conf_matrix_normalized = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "   ```\n",
    "\n",
    "3. **Adjust Decision Threshold:**\n",
    "- **Receiver Operating Characteristic (ROC) Curve:** Analyze the ROC curve to visualize the trade-off between true positive rate and false positive rate across different decision thresholds.\n",
    "   ```python\n",
    "   from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "   fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "   roc_auc = auc(fpr, tpr)\n",
    "\n",
    "   plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}')\n",
    "   plt.xlabel('False Positive Rate')\n",
    "   plt.ylabel('True Positive Rate')\n",
    "   plt.legend()\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "- **Adjust Decision Threshold:** Depending on the balance between precision and recall needed, you can adjust the decision threshold to achieve the desired trade-off.\n",
    "\n",
    "4. **Resampling Techniques:**\n",
    "- **Over-sampling Minority Class:** Use over-sampling techniques (e.g., SMOTE) to balance the class distribution and help the model better learn the minority class.\n",
    "   ```python\n",
    "   from imblearn.over_sampling import SMOTE\n",
    "\n",
    "   smote = SMOTE(sampling_strategy='auto')\n",
    "   X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "- **Under-sampling Majority Class:** Use under-sampling techniques to reduce the number of instances in the majority class.\n",
    "   ```python\n",
    "   from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "   rus = RandomUnderSampler(sampling_strategy='auto')\n",
    "   X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "5. **Model Selection:**\n",
    "- **Use Robust Models:** Choose models that are robust to imbalanced datasets, such as ensemble methods (e.g., Random Forest, Gradient Boosting) or algorithms that allow class weights.\n",
    "\n",
    "- **Class Weights:** Assign higher weights to the minority class during model training.\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "   class_weight = {0: 1, 1: 10}  # Adjust the weights based on the imbalance ratio\n",
    "\n",
    "   model = RandomForestClassifier(class_weight=class_weight)\n",
    "   ```\n",
    "\n",
    "6. **Cross-Validation:**\n",
    "- **Stratified Cross-Validation:** When performing cross-validation, use stratified sampling to ensure that each fold maintains the class distribution of the original dataset.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "   skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2aba04",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d71a3a",
   "metadata": {},
   "source": [
    "Dealing with imbalanced datasets, where one class is significantly more prevalent than the other, is a common challenge in machine learning. In the context of estimating customer satisfaction for a project, if the dataset is unbalanced with the majority of customers reporting satisfaction, you might want to balance the dataset by down-sampling the majority class.\n",
    "\n",
    "1. **Under-sampling (Down-sampling):**\n",
    "   - **Random Under-sampling:** Randomly remove samples from the majority class until it is balanced with the minority class.\n",
    "   - **Cluster Centroids:** Use clustering techniques to create centroids based on clusters of the majority class, reducing the number of majority class instances.\n",
    "   - **NearMiss:** Select samples from the majority class that are close to the minority class.\n",
    "\n",
    "2. **Synthetic Data Generation:**\n",
    "   - **SMOTE (Synthetic Minority Over-sampling Technique):** Create synthetic samples for the minority class to balance the dataset. This involves generating synthetic instances along the line segments joining existing minority class instances.\n",
    "   - **ADASYN (Adaptive Synthetic Sampling):** Similar to SMOTE but places more synthetic samples in regions where the class distribution is sparser.\n",
    "\n",
    "3. **Ensemble Methods:**\n",
    "   - Use ensemble methods that are robust to class imbalance, such as Random Forest or XGBoost. These algorithms inherently handle imbalanced datasets well.\n",
    "\n",
    "4. **Evaluation Metrics:**\n",
    "   - Choose appropriate evaluation metrics that are sensitive to the minority class, such as precision, recall, F1-score, or area under the precision-recall curve.\n",
    "\n",
    "When applying these methods, it's crucial to split your dataset into training and testing sets before any sampling to avoid data leakage. Also, be cautious of potential information loss when down-sampling, as you may discard valuable information from the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8512d",
   "metadata": {},
   "source": [
    "### 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbcb6d",
   "metadata": {},
   "source": [
    "When dealing with imbalanced datasets where the minority class is underrepresented, and you need to estimate the occurrence of a rare event, you can employ several methods to balance the dataset and up-sample the minority class. Here are some commonly used techniques:\n",
    "\n",
    "1. **Over-sampling (Up-sampling):**\n",
    "   - **Random Over-sampling:** Randomly duplicate instances from the minority class to increase its representation in the dataset.\n",
    "   - **SMOTE (Synthetic Minority Over-sampling Technique):** As mentioned earlier, SMOTE creates synthetic samples for the minority class by generating new instances along the line segments joining existing minority class instances.\n",
    "   - **ADASYN (Adaptive Synthetic Sampling):** Similar to SMOTE but adapts the synthesis of samples based on the data distribution.\n",
    "\n",
    "2. **Bootstrap Sampling:**\n",
    "   - **Bootstrapping:** Randomly sample instances with replacement from the minority class to create new samples.\n",
    "\n",
    "3. **Ensemble Methods:**\n",
    "   - **EasyEnsemble:** This technique builds multiple classifiers on different balanced subsets of the original dataset and combines their predictions.\n",
    "   - **BalanceCascade:** Another ensemble approach that repeatedly trains and removes misclassified instances from the majority class, making the dataset more balanced.\n",
    "\n",
    "4. **Weighted Algorithms:**\n",
    "   - Many machine learning algorithms allow you to assign weights to different classes. Increasing the weight of the minority class during training can help the algorithm give more importance to it.\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - For certain types of data (e.g., image data), you can apply data augmentation techniques to create variations of the minority class instances.\n",
    "\n",
    "When applying these methods, it's crucial to split your dataset into training and testing sets before any sampling to avoid data leakage. Also, be cautious of potential overfitting and increased model complexity when up-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbb897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
